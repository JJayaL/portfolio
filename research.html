<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Deep Learning-based Grasp Quality Classifier Research by Jayalekshmi Jayakumar">
    <meta name="keywords"
        content="robotics, computer vision, AI, machine learning, robotic grasping, deep learning, research">
    <meta name="author" content="Jayalekshmi Jayakumar">
    <title>Research | Deep Learning-based Grasp Quality Classifier</title>

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Space+Grotesk:wght@500;600;700&display=swap"
        rel="stylesheet">

    <!-- Stylesheet -->
    <link rel="stylesheet" href="style.css">

    <!-- Custom styles for research page - disable tilt effect -->
    <style>
        .card {
            transform: none !important;
            transition: none !important;
        }

        .card:hover {
            transform: none !important;
        }
    </style>
</head>

<body>
    <!-- Navigation -->
    <nav class="navbar" id="navbar">
        <div class="container">
            <a href="index.html" class="navbar-brand">JJ</a>
            <div style="display: flex; align-items: center; gap: 1rem;">
                <button class="theme-toggle" id="theme-toggle" aria-label="Toggle Dubai theme"
                    title="Toggle Dubai theme">
                    üèúÔ∏è
                </button>
                <button class="navbar-toggle" id="navbar-toggle" aria-label="Toggle navigation">
                    ‚ò∞
                </button>
            </div>
            <ul class="navbar-menu" id="navbar-menu">
                <li><a href="index.html#home" class="nav-link">Home</a></li>
                <li><a href="index.html#about" class="nav-link">About</a></li>
                <li><a href="index.html#education" class="nav-link">Education</a></li>
                <li><a href="index.html#experience" class="nav-link">Experience</a></li>
                <li><a href="index.html#projects" class="nav-link">Projects</a></li>
                <li><a href="index.html#contact" class="nav-link">Contact</a></li>
            </ul>
        </div>
    </nav>

    <!-- Hero Section -->
    <section class="hero section" style="min-height: 40vh; padding-top: 120px;">
        <div class="hero-content container">
            <h1 class="hero-title">Research Explained</h1>
            <p class="hero-description">
                <span class="highlight">Deep Learning-based Grasp Quality Classifier</span>
            </p>
            <p style="max-width: 800px; margin: 0 auto; color: var(--color-text-secondary);">
                A geometry-driven approach to robust robotic grasping of novel objects using minimal input
                representation.
            </p>
        </div>
    </section>

    <!-- Research Content -->
    <section class="section" id="research-content">
        <div class="container">

            <!-- 1. Goal -->
            <div class="card fade-in" style="margin-bottom: 2rem;">
                <div class="card-header">
                    <h2 class="card-title">1. What I am trying to achieve</h2>
                </div>
                <div class="card-body">
                    <p>I am developing a deep learning‚Äìbased grasp quality classifier that predicts whether a pair of
                        local surface patches is a good grasp location (prediction ‚Üí 0) or a bad grasp location
                        (prediction ‚Üí 1). The ultimate goal is to deploy this on a real robot manipulator to grasp
                        novel, unseen objects robustly.</p>

                    <p>Instead of using images or large grasp datasets, I propose a <strong>geometry-based
                            representation</strong>: the model is given only the XYZ coordinates inside two 32√ó32 local
                        patches of the object surface.</p>

                    <p>These patches come from:</p>
                    <ul>
                        <li>The same camera</li>
                        <li>Or two different, opposite cameras</li>
                    </ul>
                    <p>Crucially, they are always expressed in the same coordinate frame, so the geometry is consistent.
                        This is a minimal-input, low-dimensional, and highly interpretable representation.</p>
                </div>
            </div>

            <!-- 2. Dataset -->
            <div class="card fade-in" style="margin-bottom: 2rem;">
                <div class="card-header">
                    <h2 class="card-title">2. How the dataset is built</h2>
                </div>
                <div class="card-body">
                    <h3>Training dataset</h3>
                    <p>I manually created a synthetic dataset consisting of:</p>
                    <ul>
                        <li>Local 32√ó32 surface patches represented purely as XYZ coordinates</li>
                        <li>Each training example contains two patches</li>
                        <li>Each pair is labeled: <strong>0 ‚Üí good grasp</strong>, <strong>1 ‚Üí bad grasp</strong></li>
                    </ul>
                    <p>Thus, the model learns directly from local geometry, not pixel intensities or full object models.
                    </p>

                    <div style="margin-top: 2rem;">
                        <h4 style="margin-bottom: 1rem; color: var(--color-accent-primary);">Training Dataset Examples
                        </h4>
                        <div class="grid grid-2" style="gap: 2rem; margin-bottom: 2rem;">
                            <div>
                                <h5 style="color: #4ade80; margin-bottom: 0.5rem;">‚úì Good Grasp Pairs</h5>
                                <p style="font-size: 0.9rem; color: var(--color-text-secondary); margin-bottom: 1rem;">
                                    These patch pairs represent stable grasp locations with compatible surface
                                    geometries.
                                </p>
                                <div style="display: flex; gap: 1rem; flex-wrap: wrap;">
                                    <img src="images/good_pair_1.png" alt="Good grasp pair example 1"
                                        style="max-width: 100%; height: auto; border-radius: 8px; border: 2px solid #4ade80;">
                                    <img src="images/good_pair_2.png" alt="Good grasp pair example 2"
                                        style="max-width: 100%; height: auto; border-radius: 8px; border: 2px solid #4ade80;">
                                </div>
                            </div>
                            <div>
                                <h5 style="color: #f87171; margin-bottom: 0.5rem;">‚úó Bad Grasp Pairs</h5>
                                <p style="font-size: 0.9rem; color: var(--color-text-secondary); margin-bottom: 1rem;">
                                    These patch pairs represent unstable configurations with incompatible surface
                                    geometries.
                                </p>
                                <div style="display: flex; gap: 1rem; flex-wrap: wrap;">
                                    <img src="images/bad_pair_1.png" alt="Bad grasp pair example 1"
                                        style="max-width: 100%; height: auto; border-radius: 8px; border: 2px solid #f87171;">
                                    <img src="images/bad_pair_2.png" alt="Bad grasp pair example 2"
                                        style="max-width: 100%; height: auto; border-radius: 8px; border: 2px solid #f87171;">
                                </div>
                            </div>
                        </div>
                    </div>

                    <h3 style="margin-top: 1.5rem;">Testing dataset (unseen synthetic domain)</h3>
                    <p>I created a second dataset using <strong>BlenderProc</strong>:</p>
                    <ul>
                        <li>A single object (ape) placed in different poses, backgrounds, and scenes</li>
                        <li>Randomly sampled 100 points per camera view</li>
                        <li>Extracted a 32√ó32 spatial patch around each sampled point</li>
                    </ul>

                    <div style="margin-top: 2rem;">
                        <h4 style="margin-bottom: 1rem; color: var(--color-accent-primary);">Testing Dataset
                            Visualization</h4>

                        <!-- Point Sampling -->
                        <div style="margin-bottom: 2rem;">
                            <h5 style="margin-bottom: 0.5rem;">Step 1: Random Point Sampling from Camera Views</h5>
                            <p style="font-size: 0.9rem; color: var(--color-text-secondary); margin-bottom: 1rem;">
                                100 points are randomly sampled on the ape object from left and right camera
                                perspectives.
                            </p>
                            <div style="display: flex; gap: 1rem; flex-wrap: wrap; justify-content: center;">
                                <img src="images/test_cam_left.png" alt="Points sampled from left camera view"
                                    style="max-width: 45%; height: auto; border-radius: 8px; border: 2px solid var(--color-accent-primary);">
                                <img src="images/test_cam_right.png" alt="Points sampled from right camera view"
                                    style="max-width: 45%; height: auto; border-radius: 8px; border: 2px solid var(--color-accent-primary);">
                            </div>
                        </div>

                        <!-- Patch Extraction -->
                        <div style="margin-bottom: 2rem;">
                            <h5 style="margin-bottom: 0.5rem;">Step 2: Patch Extraction</h5>
                            <p style="font-size: 0.9rem; color: var(--color-text-secondary); margin-bottom: 1rem;">
                                Around each sampled point, a 32√ó32 local surface patch is extracted. These patches are
                                what the model is tested on.
                            </p>
                            <div style="display: flex; gap: 1rem; flex-wrap: wrap; justify-content: center;">
                                <img src="images/test_patch_1.png" alt="Extracted patch example 1"
                                    style="max-width: 45%; height: auto; border-radius: 8px; border: 2px solid var(--color-accent-secondary);">
                                <img src="images/test_patch_2.png" alt="Extracted patch example 2"
                                    style="max-width: 45%; height: auto; border-radius: 8px; border: 2px solid var(--color-accent-secondary);">
                            </div>
                        </div>

                        <!-- Model Predictions -->
                        <div style="margin-bottom: 2rem;">
                            <h5 style="margin-bottom: 0.5rem;">Step 3: Model Predictions</h5>
                            <p style="font-size: 0.9rem; color: var(--color-text-secondary); margin-bottom: 1rem;">
                                The pretrained model evaluates all pairwise combinations zero-shot and predicts grasp
                                quality.
                            </p>
                            <div style="max-width: 800px; margin: 0 auto;">
                                <video controls
                                    style="width: 100%; border-radius: 8px; border: 2px solid var(--color-accent-primary);">
                                    <source src="images/prediction_demo.webm" type="video/webm">
                                    Your browser does not support the video tag.
                                </video>
                                <p
                                    style="font-size: 0.85rem; color: var(--color-text-secondary); margin-top: 0.5rem; text-align: center;">
                                    Real-time prediction showing the model's grasp quality classification
                                </p>
                            </div>
                        </div>
                    </div>

                    <p>Then I created all pairwise combinations:</p>
                    <ul>
                        <li>Patch from cam1 paired with patch from cam2</li>
                        <li>Patch from cam1 paired with another patch from cam1</li>
                        <li>Patch from cam2 paired with another patch from cam2</li>
                    </ul>
                    <p>Again, everything is expressed in the same coordinate system, ensuring geometric consistency.
                        This forms the test pairs on which the pretrained model is evaluated‚Äîzero-shot, without
                        retraining.</p>
                </div>
            </div>

            <!-- 3. Approach -->
            <div class="card fade-in" style="margin-bottom: 2rem;">
                <div class="card-header">
                    <h2 class="card-title">3. Why this approach is different</h2>
                </div>
                <div class="card-body">
                    <div class="grid grid-2">
                        <div>
                            <h3>Traditional grasp detection models:</h3>
                            <ul>
                                <li>Use RGB or RGB-D images</li>
                                <li>Rely on large, annotated datasets</li>
                                <li>Use high-dimensional grasp parameter labels</li>
                                <li>Often need full 6DoF object pose or multiview reconstructions</li>
                            </ul>
                        </div>
                        <div>
                            <h3>My approach:</h3>
                            <ul>
                                <li>Uses only local geometry (XYZ patches)</li>
                                <li>Requires minimal annotation</li>
                                <li>Is object-agnostic</li>
                                <li>Generalizes to unseen shapes</li>
                                <li>Works under limited sensing (single/stereo camera)</li>
                                <li>Is physically interpretable</li>
                            </ul>
                        </div>
                    </div>
                    <p style="margin-top: 1rem;">This is a geometry-driven, data-efficient alternative to
                        appearance-based grasp learning.</p>
                </div>
            </div>

            <!-- 4. Learning Paradigms -->
            <div class="card fade-in" style="margin-bottom: 2rem;">
                <div class="card-header">
                    <h2 class="card-title">4. What kind of learning is involved</h2>
                </div>
                <div class="card-body">
                    <p>My pipeline represents several learning paradigms simultaneously:</p>

                    <div style="margin-bottom: 1.5rem;">
                        <h3>(A) Supervised Learning</h3>
                        <p>During training, I explicitly provide:</p>
                        <ul>
                            <li>Input = two XYZ patches</li>
                            <li>Label = ‚Äúgood‚Äù (0) or ‚Äúbad‚Äù (1) grasp</li>
                        </ul>
                        <p>So fundamentally, the model is trained using binary supervised classification.</p>
                    </div>

                    <div style="margin-bottom: 1.5rem;">
                        <h3>(B) Cross-domain Generalization / Domain Transfer</h3>
                        <p>I train on one synthetic dataset and test on a different synthetic dataset (BlenderProc ape
                            object), with no fine-tuning.</p>
                        <p>This is domain transfer or cross-domain generalization, because:</p>
                        <ul>
                            <li>Different object</li>
                            <li>Different scenes</li>
                            <li>Different rendering</li>
                            <li>Different noise and geometry distribution</li>
                        </ul>
                        <p>My model tries to generalize based solely on geometry.</p>
                    </div>

                    <div style="margin-bottom: 1.5rem;">
                        <h3>(C) Zero-shot Learning (strictly speaking, zero-shot domain transfer)</h3>
                        <p>I test directly on unseen objects & scenes without retraining. This qualifies as zero-shot
                            domain generalization, because:</p>
                        <ul>
                            <li>The model sees a new domain</li>
                            <li>With different geometry distribution</li>
                            <li>No labels or adaptation steps</li>
                        </ul>
                    </div>

                    <div>
                        <h3>(D) Geometry-based representation learning</h3>
                        <p>My model implicitly learns:</p>
                        <ul>
                            <li>Geometry</li>
                            <li>Local curvature</li>
                            <li>Flatness</li>
                            <li>Surface shape compatibility between two patches</li>
                        </ul>
                        <p>This is not image-based learning but local surface feature learning.</p>
                    </div>
                </div>
            </div>

            <!-- 5. Summary -->
            <div class="card fade-in">
                <div class="card-header">
                    <h2 class="card-title">5. High-level summary</h2>
                </div>
                <div class="card-body">
                    <p>I am creating a deep learning model that predicts grasp quality using only the raw XYZ
                        coordinates of two small surface patches. I train it on a synthetic dataset of labeled patch
                        pairs, then test it zero-shot on a different synthetic dataset to evaluate generalization.</p>

                    <p>Once validated, I aim to deploy it on a real robot to infer grasp success purely from localized
                        geometry, making it:</p>
                    <div class="tags">
                        <span class="tag">Interpretable</span>
                        <span class="tag">Efficient</span>
                        <span class="tag">Generalizable</span>
                        <span class="tag">Suitable for Novel Objects</span>
                    </div>

                    <p style="margin-top: 1rem;">This is a meaningful step toward geometry-driven robotic grasping that
                        does not depend on massive datasets or high-dimensional labels.</p>
                </div>
            </div>

            <div style="text-align: center; margin-top: 3rem;">
                <a href="index.html#experience" class="btn btn-primary">Back to Experience</a>
            </div>

        </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>&copy; 2025 Jayalekshmi Jayakumar. All rights reserved.</p>
            <p>Built with passion for robotics and innovation ü§ñ</p>
        </div>
    </footer>

    <!-- JavaScript -->
    <script src="script.js?v=3"></script>
</body>

</html>